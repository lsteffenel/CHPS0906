{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/CHPS0906/blob/main/TP1/3-R%C3%A9seau%20de%20neurones%20en%20PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar1OeBh9Hqv5"
      },
      "source": [
        "# Réseaux de Neurones avec PyTorch\n",
        "\n",
        "Les réseaux d'apprentissage profond ont tendance à être massifs avec des dizaines ou des centaines de couches, c'est de là que vient le terme « profond ». Vous pouvez construire l'un de ces réseaux profonds en utilisant uniquement des matrices de pondération comme nous l'avons fait dans le notebook précédent, mais en général, c'est très lourd et difficile à mettre en œuvre. PyTorch dispose d'un module intéressant, `nn`, qui fournit un moyen pratique de construire efficacement de grands réseaux de neurones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7mWghGNVHqv6"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import helper\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxTXdc83Hqv6"
      },
      "source": [
        "Nous allons maintenant construire un réseau plus important qui peut résoudre un problème (auparavant) difficile, à savoir identifier du texte dans une image. Ici, nous utiliserons l'ensemble de données MNIST qui se compose de chiffres manuscrits en niveaux de gris. Chaque image mesure 28x28 pixels, vous pouvez voir un échantillon ci-dessous\n",
        "\n",
        "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/mnist.png?raw=1'>\n",
        "\n",
        "Notre objectif est de construire un réseau de neurones capable de prendre l'une de ces images et de prédire le chiffre affiché.\n",
        "\n",
        "Tout d'abord, nous devons obtenir notre ensemble de données. Il est fourni via le package `torchvision`. Le code ci-dessous téléchargera l'ensemble de données MNIST, puis créera des ensembles de données d'entraînement et de test pour nous.\n",
        "\n",
        "Probablement il y aura des messages d'erreur car le serveur de stockage a une protection CloudFlare. Cependant, une fois executé, le dataset doit être disponible (vous le verrez dans les paragraphes suivants)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djQJkieYHqv6"
      },
      "outputs": [],
      "source": [
        "# The MNIST datasets are hosted on yann.lecun.com that has moved under CloudFlare protection\n",
        "# Run this script to enable the datasets download\n",
        "# Reference: https://github.com/pytorch/vision/issues/1938\n",
        "\n",
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5VR5AEX-Hqv6"
      },
      "outputs": [],
      "source": [
        "### Run this cell\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ailbIRY4Hqv7"
      },
      "source": [
        "Nous avons les données d'entraînement chargées dans `trainloader` et nous en faisons un itérateur avec `iter(trainloader)`. Plus tard, nous l'utiliserons pour parcourir l'ensemble de données pour l'entraînement, comme\n",
        "\n",
        "```python\n",
        "for image, label in trainloader:\n",
        "## do things with images and labels\n",
        "```\n",
        "\n",
        "Vous remarquerez que nous avons créé le `trainloader` avec une taille de *batch* de 64 et `shuffle=True`. La taille du batch est le nombre d'images que nous obtenons en une itération à partir du chargeur de données et que nous transmettons via notre réseau, souvent appelé un *batch*. Et `shuffle=True` lui indique de mélanger l'ensemble de données chaque fois que nous recommençons à parcourir le dataloader. Mais ici, nous récuperons simplement le premier batch afin que nous puissions vérifier les données. Nous pouvons voir ci-dessous que `images` n'est qu'un tensor de taille `(64, 1, 28, 28)`. Donc, 64 images par batch, 1 canal de couleur et 28x28 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XwXC1NsJHqv7"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ZPaPYTHqv7"
      },
      "source": [
        "Voici ce que cette image ressemble :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Kc-Q7GgbHqv7"
      },
      "outputs": [],
      "source": [
        "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYyLqJE5Hqv7"
      },
      "source": [
        "Commençons par construire un réseau simple pour cet ensemble de données en utilisant des matrices poids et des multiplications de matrices. Ensuite, nous verrons comment le faire en utilisant le module `nn` de PyTorch qui fournit une méthode beaucoup plus pratique et puissante pour définir des architectures de réseau.\n",
        "\n",
        "Les réseaux que vous avez vus jusqu'à présent sont appelés réseaux *entièrement connectés* ou *denses*. Chaque unité d'une couche est connectée à chaque unité de la couche suivante. Dans les réseaux entièrement connectés, l'entrée de chaque couche doit être un vecteur unidimensionnel (qui peut être empilé dans un tenseur 2D sous forme de *batches* de plusieurs exemples). Cependant, nos images sont des tensors 2D 28x28, nous devons donc les convertir en vecteurs 1D. En pensant aux tailles, nous devons convertir le batch d'images de forme `(64, 1, 28, 28)` en un batch d'images de forme `(64, 784)`, 784 étant 28 fois 28. Ceci est généralement appelé *aplatissement*, nous devons donc aplatir les images 2D en vecteurs 1D.\n",
        "\n",
        "Auparavant, vous avez construit un réseau avec une unité (neurone) de sortie. Ici, nous avons besoin de 10 unités de sortie, une pour chaque chiffre. Nous voulons que notre réseau prédise le chiffre affiché dans une image, il faudra donc calculer les probabilités que l'image soit d'un chiffre ou d'une classe quelconque. Cela finit par être une distribution de probabilité discrète sur les classes (chiffres) qui nous indique la classe la plus probable pour l'image. Cela signifie que nous avons besoin de 10 unités de sortie pour les 10 classes (chiffres). Nous verrons ensuite comment convertir la sortie du réseau en une distribution de probabilité.\n",
        "\n",
        "> **Exercice :** Aplatir le batch d'images `images`. Créez ensuite un réseau multicouche avec 784 unités d'entrée, 256 unités cachées et 10 unités de sortie en utilisant des tensors aléatoires pour les poids et les biais. Pour l'instant, utilisez une activation sigmoïde pour la couche cachée. Laissez la couche de sortie sans activation, nous en ajouterons une qui nous donnera une distribution de probabilité ensuite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gClcGRMBHqv7"
      },
      "outputs": [],
      "source": [
        "## Votre solution\n",
        "\n",
        "\n",
        "out = # la sortie du réseau doit avoir la forme (64,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqb919L7Hqv7"
      },
      "source": [
        "Nous avons maintenant 10 sorties pour notre réseau. Nous voulons transmettre une image à notre réseau et obtenir une distribution de probabilité sur les classes qui nous indique à quelle(s) classe(s) l'image appartient probablement. Quelque chose qui ressemble à ceci :\n",
        "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/image_distribution.png?raw=1' width=500px>\n",
        "\n",
        "Ici, nous voyons que la probabilité pour chaque classe est à peu près la même. Cela représente un réseau non formé, il n'a pas encore vu de données, il renvoie donc simplement une distribution uniforme avec des probabilités égales pour chaque classe.\n",
        "\n",
        "Pour calculer cette distribution de probabilité, nous utilisons souvent la [**fonction softmax**](https://en.wikipedia.org/wiki/Softmax_function). Mathématiquement, cela ressemble à\n",
        "\n",
        "$$\n",
        "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
        "$$\n",
        "\n",
        "Ce que cela fait, c'est écraser chaque entrée $x_i$ entre 0 et 1 et normaliser les valeurs pour vous donner une distribution de probabilité appropriée où les probabilités s'additionnent à un.\n",
        "\n",
        "> **Exercice :** Implémentez une fonction `softmax` qui effectue le calcul softmax et renvoie les distributions de probabilité pour chaque exemple du batch. Notez que vous devrez faire attention aux formes lorsque vous faites cela. Si vous avez un tenseur `a` de forme `(64, 10)` et un tenseur `b` de forme `(64,)`, faire `a/b` vous donnera une erreur car PyTorch essaiera de faire la division sur les colonnes (appelée diffusion) mais vous obtiendrez une incompatibilité de taille.\n",
        "\n",
        "La façon de voir les choses est la suivante : pour chacun des 64 exemples, vous ne voulez diviser que par une valeur, la somme du dénominateur. Vous avez donc besoin que `b` ait une forme de `(64, 1)`. De cette façon, PyTorch divisera les 10 valeurs de chaque ligne de `a` par la valeur de chaque ligne de `b`. Faites également attention à la façon dont vous prenez la somme. Vous devrez définir le mot-clé `dim` dans `torch.sum`. Le réglage `dim=0` prend la somme sur les lignes tandis que `dim=1` prend la somme sur les colonnes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UvTteV0QHqv7"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    ## TODO: implémenter la fonction softmax ici\n",
        "\n",
        "# Here, out should be the output of the network in the previous excercise with shape (64,10)\n",
        "probabilities = softmax(out)\n",
        "\n",
        "# Does it have the right shape? Should be (64, 10)\n",
        "print(probabilities.shape)\n",
        "# Does it sum to 1?\n",
        "print(probabilities.sum(dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B93BF9LHqv7"
      },
      "source": [
        "## Création de réseaux avec PyTorch\n",
        "\n",
        "PyTorch fournit un module `nn` qui simplifie considérablement la création de réseaux. Voyons comment créer le même module que ci-dessus avec 784 entrées, 256 unités cachées, 10 unités de sortie et une sortie softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "olNFEBY5Hqv7"
      },
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1RTujIuGHqv7"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.hidden = nn.Linear(784, 256)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.output = nn.Linear(256, 10)\n",
        "\n",
        "        # Define sigmoid activation and softmax output\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.hidden(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.output(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2FEduUkHqv8"
      },
      "source": [
        "Passons en revue cela petit à petit.\n",
        "\n",
        "```python\n",
        "class Network(nn.Module):\n",
        "```\n",
        "\n",
        "Ici, nous héritons de `nn.Module`. Combiné avec `super().__init__()`, cela crée une classe qui suit l'architecture et fournit de nombreuses méthodes et attributs utiles. Il est obligatoire d'hériter de `nn.Module` lorsque vous créez une classe pour votre réseau. Le nom de la classe elle-même peut être n'importe quoi.\n",
        "\n",
        "```python\n",
        "self.hidden = nn.Linear(784, 256)\n",
        "```\n",
        "\n",
        "Cette ligne crée un module pour une transformation linéaire, $x\\mathbf{W} + b$, avec 784 entrées et 256 sorties et l'affecte à `self.hidden`. Le module crée automatiquement les tensors de poids et de biais que nous utiliserons dans la méthode `forward`. Vous pouvez accéder aux tensors de poids et de biais une fois que le réseau (`net`) est créé avec `net.hidden.weight` et `net.hidden.bias`.\n",
        "\n",
        "```python\n",
        "self.output = nn.Linear(256, 10)\n",
        "```\n",
        "\n",
        "De même, cela crée une autre transformation linéaire avec 256 entrées et 10 sorties.\n",
        "\n",
        "```python\n",
        "self.sigmoid = nn.Sigmoid()\n",
        "self.softmax = nn.Softmax(dim=1)\n",
        "```\n",
        "\n",
        "Ici, on a définit des opérations pour l'activation sigmoïde et la sortie softmax. La définition de `dim=1` dans `nn.Softmax(dim=1)` calcule le softmax sur les colonnes.\n",
        "\n",
        "```python\n",
        "def forward(self, x):\n",
        "```\n",
        "\n",
        "Les réseaux PyTorch créés avec `nn.Module` doivent avoir une méthode `forward` définie. Il prend un tensor `x` et le transmet aux opérations que vous avez définies dans la méthode `__init__`.\n",
        "\n",
        "```python\n",
        "x = self.hidden(x)\n",
        "x = self.sigmoid(x)\n",
        "x = self.output(x)\n",
        "x = self.softmax(x)\n",
        "```\n",
        "\n",
        "Ici, le tensor d'entrée `x` est transmis à chaque opération et réaffecté à `x`. Nous pouvons voir que le tensor d'entrée passe par la couche cachée, puis une fonction sigmoïde, puis la couche de sortie et enfin la fonction softmax. Peu importe le nom que vous donnez aux variables ici, tant que les entrées et les sorties des opérations correspondent à l'architecture réseau que vous souhaitez créer. L'ordre dans lequel vous définissez les éléments dans la méthode `__init__` n'a pas d'importance, mais vous devrez séquencer correctement les opérations dans la méthode `forward`.\n",
        "\n",
        "Nous pouvons maintenant créer un objet `Network`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_bel4Y8ZHqv8"
      },
      "outputs": [],
      "source": [
        "# Create the network and look at it's text representation\n",
        "model = Network()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOyP8OvwHqv8"
      },
      "source": [
        "You can define the network somewhat more concisely and clearly using the `torch.nn.functional` module. This is the most common way you'll see networks defined as many operations are simple element-wise functions. We normally import this module as `F`, `import torch.nn.functional as F`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1D5ut6LTHqv8"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.hidden = nn.Linear(784, 256)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.output = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Hidden layer with sigmoid activation\n",
        "        x = F.sigmoid(self.hidden(x))\n",
        "        # Output layer with softmax activation\n",
        "        x = F.softmax(self.output(x), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njomAnpWHqv8"
      },
      "source": [
        "### Fonctions d'activation\n",
        "\n",
        "Jusqu'ici, nous n'avons étudié que la fonction d'activation sigmoïde, mais en général, n'importe quelle fonction peut être utilisée comme fonction d'activation. La seule exigence est que pour qu'un réseau puisse approximer une fonction non linéaire, les fonctions d'activation doivent être non linéaires. Voici quelques exemples supplémentaires de fonctions d'activation courantes : Tanh (tangente hyperbolique) et ReLU (unité linéaire rectifiée).\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/activation.png?raw=1\" width=700px>\n",
        "\n",
        "En pratique, la fonction ReLU est la plus utilisée comme fonction d'activation pour les couches cachées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJnAjGr9Hqv8"
      },
      "source": [
        "### À vous de construire un réseau\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/mlp_mnist.png?raw=1\" width=600px>\n",
        "\n",
        "> **Exercice :** Créez un réseau avec 784 unités d'entrée, une couche cachée avec 128 unités et une activation ReLU, puis une couche cachée avec 64 unités et une activation ReLU, et enfin une couche de sortie avec une activation softmax comme indiqué ci-dessus. Vous pouvez utiliser une activation ReLU avec le module `nn.ReLU` ou la fonction `F.relu`.\n",
        "\n",
        "Il est recommandé de nommer vos couches par leur type de réseau, par exemple 'fc' pour représenter une couche entièrement connectée. Lorsque vous codez votre solution, utilisez `fc1`, `fc2` et `fc3` comme noms de vos couches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "oG-HBJ_CHqv8"
      },
      "outputs": [],
      "source": [
        "## Votre solution ici\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLP3O0wHHqv8"
      },
      "source": [
        "### Initialisation des poids et des biais\n",
        "\n",
        "Les poids et autres sont automatiquement initialisés pour vous, mais il est possible de personnaliser la manière dont ils sont initialisés. Les poids et les biais sont des tensors attachés à la couche que vous avez définie, vous pouvez les obtenir avec `model.fc1.weight` par exemple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nDQmEjRGHqv8"
      },
      "outputs": [],
      "source": [
        "print(model.fc1.weight)\n",
        "print(model.fc1.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJo66q7tHqv8"
      },
      "source": [
        "Pour une initialisation personnalisée, nous souhaitons modifier ces tensors en place. Ce sont en fait des *variables* autograd, nous devons donc récupérer les tensors réels avec `model.fc1.weight.data`. Une fois que nous avons les tensors, nous pouvons les remplir avec des zéros (pour les biais) ou des valeurs normales aléatoires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IJdeBkF2Hqv8"
      },
      "outputs": [],
      "source": [
        "# Set biases to all zeros\n",
        "model.fc1.bias.data.fill_(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YixISiajHqv8"
      },
      "outputs": [],
      "source": [
        "# sample from random normal with standard dev = 0.01\n",
        "model.fc1.weight.data.normal_(std=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAo4M_OQHqv8"
      },
      "source": [
        "### Forward pass\n",
        "\n",
        "Maintenant que nous avons un réseau, voyons ce qui se passe lorsque nous transmettons une image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iaOYSEKMHqv8"
      },
      "outputs": [],
      "source": [
        "def view_classify(img, ps, version=\"MNIST\"):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    if version == \"MNIST\":\n",
        "        ax2.set_yticklabels(np.arange(10))\n",
        "    elif version == \"Fashion\":\n",
        "        ax2.set_yticklabels(['T-shirt/top',\n",
        "                            'Trouser',\n",
        "                            'Pullover',\n",
        "                            'Dress',\n",
        "                            'Coat',\n",
        "                            'Sandal',\n",
        "                            'Shirt',\n",
        "                            'Sneaker',\n",
        "                            'Bag',\n",
        "                            'Ankle Boot'], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Grab some data\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels)\n",
        "images.resize_(64, 1, 784)\n",
        "# or images.resize_(images.shape[0], 1, 784) to automatically get batch size\n",
        "\n",
        "# Forward pass through the network\n",
        "img_idx = 0\n",
        "ps = model.forward(images[img_idx,:])\n",
        "\n",
        "img = images[img_idx]\n",
        "view_classify(img.view(1, 28, 28), ps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVwiPtO5Hqv9"
      },
      "source": [
        "Comme vous pouvez le voir ci-dessus, notre réseau n'a pratiquement aucune idée de ce qu'est ce chiffre. C'est parce que nous ne l'avons pas encore formé, tous les poids sont aléatoires !\n",
        "\n",
        "### Utilisation de `nn.Sequential`\n",
        "\n",
        "PyTorch fournit un moyen pratique de créer des réseaux comme celui-ci où un tenseur est transmis séquentiellement via des opérations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). En utilisant ceci pour créer le réseau équivalent :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o8hgJMKxHqv9"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters for our network\n",
        "input_size = 784\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "\n",
        "# Build a feed-forward network\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.Softmax(dim=1))\n",
        "print(model)\n",
        "\n",
        "# Forward pass through the network and display output\n",
        "images, labels = next(iter(trainloader))\n",
        "images.resize_(images.shape[0], 1, 784)\n",
        "ps = model.forward(images[0,:])\n",
        "helper.view_classify(images[0].view(1, 28, 28), ps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCrcx9zXHqv9"
      },
      "source": [
        "Ici, notre modèle est le même que précédemment : 784 unités d'entrée, une couche cachée avec 128 unités, l'activation ReLU, une couche cachée de 64 unités, une autre ReLU, puis la couche de sortie avec 10 unités et la sortie softmax.\n",
        "\n",
        "Les opérations sont disponibles en passant l'index approprié. Par exemple, si vous souhaitez obtenir la première opération linéaire et examiner les poids, vous utiliserez `model[0]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ONYMWajKHqv9"
      },
      "outputs": [],
      "source": [
        "print(model[0])\n",
        "model[0].weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFZqBuvvHqv9"
      },
      "source": [
        "Vous pouvez également transmettre un `OrderedDict` pour nommer les couches et les opérations individuelles, au lieu d'utiliser des entiers incrémentiels. Notez que les clés du dictionnaire doivent être uniques, donc _chaque opération doit avoir un nom différent_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7tWnGUpVHqv9"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "model = nn.Sequential(OrderedDict([\n",
        "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
        "                      ('relu1', nn.ReLU()),\n",
        "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
        "                      ('relu2', nn.ReLU()),\n",
        "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
        "                      ('softmax', nn.Softmax(dim=1))]))\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLxauML1Hqv9"
      },
      "source": [
        "on peut ainsi accéder à chaque couche selon leurs noms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3gc9viCbHqv9"
      },
      "outputs": [],
      "source": [
        "print(model[0])\n",
        "print(model.fc1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-YM7gHWHqv9"
      },
      "source": [
        "Dans le prochain notebook, nous verrons comment nous pouvons entraîner un réseau neuronal à prédire avec précision les nombres apparaissant dans les images MNIST."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}